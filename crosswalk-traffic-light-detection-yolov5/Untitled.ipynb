{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b977d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12315904 parameters, 0 gradients\n",
      "Fusing layers... \n",
      "Model Summary: 280 layers, 12315904 parameters, 0 gradients\n",
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\asus\\anaconda3\\lib\\threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\asus\\anaconda3\\lib\\threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-1-590d1a2ac1a9>\", line 57, in update\n",
      "cv2.error: Unknown C++ exception from OpenCV code\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Unknown C++ exception from OpenCV code",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-590d1a2ac1a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m#   break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# cv2.imshow('result', result_img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-590d1a2ac1a9>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                 \u001b[1;31m# read the next frame from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                                 \u001b[0mgrabbed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                                 \u001b[1;31m# if the `grabbed` boolean is `False`, then we have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                                 \u001b[1;31m# reached the end of the video file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: Unknown C++ exception from OpenCV code"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.plots import Annotator\n",
    "import pickle\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "import time\n",
    "import cv2 \n",
    "# import imutils\n",
    "\n",
    "class FileVideoStream:\n",
    "\tdef __init__(self, path, queueSize=128):\n",
    "\t\t# initialize the file video stream along with the boolean\n",
    "\t\t# used to indicate if the thread should be stopped or not\n",
    "\t\tself.stream = cv2.VideoCapture('data/sample.mp4')\n",
    "\t\tself.stopped = False\n",
    "\t\t# initialize the queue used to store frames read from\n",
    "\t\t# the video file\n",
    "\t\tself.Q = Queue(maxsize=queueSize)\n",
    "\n",
    "\tdef start(self):\n",
    "\t\t# start a thread to read frames from the file video stream\n",
    "\t\tt = Thread(target=self.update, args=())\n",
    "\t\tt.daemon = True\n",
    "\t\tt.start()\n",
    "\t\treturn self\n",
    "\n",
    "\tdef update(self):\n",
    "\t\t# keep looping infinitely\n",
    "\t\tMODEL_PATH = 'runs/train/exp4/weights/best.pt'\n",
    "\n",
    "\t\timg_size = 640\n",
    "\t\tconf_thres = 0.5  # confidence threshold\n",
    "\t\tiou_thres = 0.45  # NMS IOU threshold\n",
    "\t\tmax_det = 500  # maximum detections per image\n",
    "\t\tclasses = None  # filter by class\n",
    "\t\tagnostic_nms = False  # class-agnostic NMS\n",
    "\n",
    "\t\tdevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\t\tckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "\t\tmodel = ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval()\n",
    "\t\tclass_names = ['gray', 'red', 'green'] # model.names\n",
    "\t\tstride = int(model.stride.max())\n",
    "\t\tcolors = ((50, 50, 50), (0, 0, 255), (0, 255, 0)) # (gray, red, green)\n",
    "\n",
    "\t\twhile True:\n",
    "\t\t\t# if the thread indicator variable is set, stop the\n",
    "\t\t\t# thread\n",
    "\t\t\tif self.stopped:\n",
    "\t\t\t\treturn\n",
    "\t\t\t# otherwise, ensure the queue has room in it\n",
    "\t\t\tif not self.Q.full():\n",
    "\t\t\t\t# read the next frame from the file\n",
    "\t\t\t\tgrabbed, img = self.stream.read()\n",
    "\t\t\t\t# if the `grabbed` boolean is `False`, then we have\n",
    "\t\t\t\t# reached the end of the video file\n",
    "\t\t\t\tif not grabbed:\n",
    "\t\t\t\t\tself.stop()\n",
    "\t\t\t\t\treturn\n",
    "\n",
    "\t\t\t\t# preprocess\n",
    "\t\t\t\timg_input = letterbox(img, img_size, stride=stride)[0]\n",
    "\t\t\t\timg_input = img_input.transpose((2, 0, 1))[::-1]\n",
    "\t\t\t\timg_input = np.ascontiguousarray(img_input)\n",
    "\t\t\t\timg_input = torch.from_numpy(img_input).to(device)\n",
    "\t\t\t\timg_input = img_input.float()\n",
    "\t\t\t\timg_input /= 255.\n",
    "\t\t\t\timg_input = img_input.unsqueeze(0)\n",
    "\n",
    "\t\t\t\t# inference\n",
    "\t\t\t\tpred = model(img_input, augment=False, visualize=False)[0]\n",
    "\n",
    "\t\t\t\t# postprocess\n",
    "\t\t\t\tpred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)[0]\n",
    "\n",
    "\t\t\t\tpred = pred.cpu().numpy()\n",
    "\n",
    "\t\t\t\tpred[:, :4] = scale_coords(img_input.shape[2:], pred[:, :4], img.shape).round()\n",
    "\n",
    "\t\t\t\tannotator = Annotator(img.copy(), line_width=3, example=str(class_names), font='data/malgun.ttf')\n",
    "\n",
    "\t\t\t\tfor p in pred:\n",
    "\t\t\t\t\tclass_name = class_names[int(p[5])]\n",
    "\n",
    "\t\t\t\t\tx1, y1, x2, y2 = p[:4]\n",
    "\n",
    "\t\t\t\t\tannotator.box_label([x1, y1, x2, y2], '%s %d' % (class_name, float(p[4]) * 100), color=colors[int(p[5])])\n",
    "\n",
    "\t\t\t\tresult_img = annotator.result()\n",
    "\t\n",
    "\t\t\t\t# add the frame to the queue\n",
    "\t\t\t\tself.Q.put(result_img)\n",
    "\t# def update(self):\n",
    "\t# \t# keep looping infinitely\n",
    "\t# \twhile True:\n",
    "\t# \t\t# if the thread indicator variable is set, stop the\n",
    "\t# \t\t# thread\n",
    "\t# \t\tif self.stopped:\n",
    "\t# \t\t\treturn\n",
    "\t# \t\t# otherwise, ensure the queue has room in it\n",
    "\t# \t\tif not self.Q.full():\n",
    "\t# \t\t\t# read the next frame from the file\n",
    "\t# \t\t\t(grabbed, frame) = self.stream.read()\n",
    "\t# \t\t\t# if the `grabbed` boolean is `False`, then we have\n",
    "\t# \t\t\t# reached the end of the video file\n",
    "\t# \t\t\tif not grabbed:\n",
    "\t# \t\t\t\tself.stop()\n",
    "\t# \t\t\t\treturn\n",
    "\t# \t\t\t# add the frame to the queue\n",
    "\t# \t\t\tself.Q.put(frame)\n",
    "\tdef read(self):\n",
    "\t\t# return next frame in the queue\n",
    "\t\treturn self.Q.get()\n",
    "\n",
    "\tdef more(self):\n",
    "\t\t# return True if there are still frames in the queue\n",
    "\t\treturn self.Q.qsize() > 0\n",
    "\n",
    "\tdef stop(self):\n",
    "\t\t# indicate that the thread should be stopped\n",
    "\t\tself.stopped = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# MODEL_PATH = 'runs/train/exp4/weights/best.pt'\n",
    "\n",
    "# img_size = 640\n",
    "# conf_thres = 0.5  # confidence threshold\n",
    "# iou_thres = 0.45  # NMS IOU threshold\n",
    "# max_det = 500  # maximum detections per image\n",
    "# classes = None  # filter by class\n",
    "# agnostic_nms = False  # class-agnostic NMS\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ckpt = torch.load(MODEL_PATH, map_location=device)\n",
    "# model = ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval()\n",
    "# class_names = ['gray', 'red', 'green'] # model.names\n",
    "# stride = int(model.stride.max())\n",
    "# colors = ((50, 50, 50), (0, 0, 255), (0, 255, 0)) # (gray, red, green)\n",
    "\n",
    "#cap = cv2.VideoCapture('http://192.168.0.108:8080/video/mjpeg')\n",
    "\n",
    "cap = FileVideoStream('data/sample.mp4').start()\n",
    "time.sleep(1.0)\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "#out = cv2.VideoWriter('data/output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "#out = cv2.VideoWriter('data/output.mp4', fourcc, cap.stream.get(cv2.CAP_PROP_FPS), (int(cap.stream.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.stream.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "#while cap.isOpened():\n",
    "while cap.more():\n",
    "    # ret, img = cap.read()\n",
    "    img = cap.read()\n",
    "    \n",
    "    # frame = imutils.resize(img, width=450)\n",
    "    # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # frame = np.dstack([frame, frame, frame])\n",
    "\t# display the size of the queue on the frame\n",
    "    # cv2.putText(frame, \"Queue Size: {}\".format(cap.Q.qsize()),\n",
    "\t# \t(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\t\n",
    "\n",
    "\t# show the frame and update the FPS counter\n",
    "    cv2.imshow('result', img)\n",
    "    #out.write(img)\n",
    "    # if cv2.waitKey(1) == ord('q'):\n",
    "    #   break\n",
    "    cv2.waitKey(1)\n",
    "    cap.update()\n",
    "\n",
    "    # cv2.imshow('result', result_img)\n",
    "    # #out.write(result_img)\n",
    "    # if cv2.waitKey(1) == ord('q'):\n",
    "    #     break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e014985",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
